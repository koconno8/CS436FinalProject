{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koconno8/CS436FinalProject/blob/main/mnistUpdate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#minstNetwork.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "# conv1-conv11-pool-conv2-conv21\n",
        "\n",
        "class MNIST_Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Network, self).__init__()\n",
        "        self.conv1    = nn.Conv2d(1,32,kernel_size=5,dilation=1, stride=1, padding=2,bias=True)\n",
        "        self.conv11   = nn.Conv2d(32,32,kernel_size=5,dilation=1, stride=1, padding=2,bias=True)\n",
        "        self.pool1    = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32,64,kernel_size=5,dilation=1, stride=1, padding=2,bias=True)\n",
        "        self.conv21 = nn.Conv2d(64,64,kernel_size=5,dilation=1, stride=1, padding=2,bias=True)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.fc1   = nn.Linear(64*7*7,512,bias=True )\n",
        "        self.fc2   = nn.Linear(512, 10)\n",
        "    def forward(self, input):\n",
        "        out = F.relu((self.conv1(input)))\n",
        "        out = F.relu((self.conv11(out)))\n",
        "        out = self.pool1(out)\n",
        "\n",
        "        out = F.relu((self.conv2(out)))\n",
        "        out = F.relu((self.conv21(out)))\n",
        "        out = self.pool2(out)\n",
        "\n",
        "        # fc-1\n",
        "        B,C,H,W = out.size()\n",
        "        out = out.view(B,-1)\n",
        "        out =(F.relu((self.fc1(out))))\n",
        "        # Logits\n",
        "        out = self.fc2(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "UpiChz3FyvZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_ybHr7byh0D",
        "outputId": "c7848677-a02b-47f3-808e-0470a23bae50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cudnn status: True\n",
            "MNIST dataloader: Done\n",
            "| Epoch [  0/ 50] Iter[391/390] : Loss:1.463496 \t\tEpoch: 0  Time taken: 10.456432342529297\n",
            "| Epoch [  1/ 50] Iter[391/390] : Loss:0.974477 \t\tEpoch: 1  Time taken: 10.182543754577637\n",
            "| Epoch [  2/ 50] Iter[391/390] : Loss:0.758520 \t\tEpoch: 2  Time taken: 10.31870436668396\n",
            "| Epoch [  3/ 50] Iter[391/390] : Loss:0.886610 \t\tEpoch: 3  Time taken: 10.194785118103027\n",
            "| Epoch [  4/ 50] Iter[391/390] : Loss:0.592734 \t\tEpoch: 4  Time taken: 10.431707382202148\n",
            "| Epoch [  5/ 50] Iter[391/390] : Loss:0.336903 \t\tEpoch: 5  Time taken: 10.326686382293701\n",
            "| Epoch [  6/ 50] Iter[391/390] : Loss:0.295170 \t\tEpoch: 6  Time taken: 10.270173788070679\n",
            "| Epoch [  7/ 50] Iter[391/390] : Loss:0.279707 \t\tEpoch: 7  Time taken: 10.286213874816895\n",
            "| Epoch [  8/ 50] Iter[391/390] : Loss:0.186130 \t\tEpoch: 8  Time taken: 10.193046808242798\n",
            "| Epoch [  9/ 50] Iter[391/390] : Loss:0.177573 \t\tEpoch: 9  Time taken: 10.355818033218384\n",
            "| Epoch [ 10/ 50] Iter[391/390] : Loss:0.085248 \t\tEpoch: 10  Time taken: 10.18952202796936\n",
            "| Epoch [ 11/ 50] Iter[391/390] : Loss:0.073487 \t\tEpoch: 11  Time taken: 10.338229656219482\n",
            "| Epoch [ 12/ 50] Iter[391/390] : Loss:0.088786 \t\tEpoch: 12  Time taken: 10.222164869308472\n",
            "| Epoch [ 13/ 50] Iter[391/390] : Loss:0.195972 \t\tEpoch: 13  Time taken: 10.387531995773315\n",
            "| Epoch [ 14/ 50] Iter[391/390] : Loss:0.274296 \t\tEpoch: 14  Time taken: 10.367444515228271\n",
            "| Epoch [ 15/ 50] Iter[391/390] : Loss:0.115280 \t\tEpoch: 15  Time taken: 10.167287111282349\n",
            "| Epoch [ 16/ 50] Iter[391/390] : Loss:0.077992 \t\tEpoch: 16  Time taken: 10.145249605178833\n",
            "| Epoch [ 17/ 50] Iter[391/390] : Loss:0.049617 \t\tEpoch: 17  Time taken: 10.324140787124634\n",
            "| Epoch [ 18/ 50] Iter[391/390] : Loss:0.060830 \t\tEpoch: 18  Time taken: 10.156690120697021\n",
            "| Epoch [ 19/ 50] Iter[391/390] : Loss:0.238586 \t\tEpoch: 19  Time taken: 10.366053342819214\n",
            "| Epoch [ 20/ 50] Iter[391/390] : Loss:0.093272 \t\tEpoch: 20  Time taken: 10.158560276031494\n",
            "| Epoch [ 21/ 50] Iter[391/390] : Loss:0.082434 \t\tEpoch: 21  Time taken: 10.343681573867798\n",
            "| Epoch [ 22/ 50] Iter[391/390] : Loss:0.058970 \t\tEpoch: 22  Time taken: 10.171451330184937\n",
            "| Epoch [ 23/ 50] Iter[391/390] : Loss:0.134821 \t\tEpoch: 23  Time taken: 10.31414008140564\n",
            "| Epoch [ 24/ 50] Iter[391/390] : Loss:0.267265 \t\tEpoch: 24  Time taken: 10.210923671722412\n",
            "| Epoch [ 25/ 50] Iter[391/390] : Loss:0.112618 \t\tEpoch: 25  Time taken: 10.321814060211182\n",
            "| Epoch [ 26/ 50] Iter[391/390] : Loss:0.238082 \t\tEpoch: 26  Time taken: 10.3069589138031\n",
            "| Epoch [ 27/ 50] Iter[391/390] : Loss:0.157678 \t\tEpoch: 27  Time taken: 10.151607751846313\n",
            "| Epoch [ 28/ 50] Iter[391/390] : Loss:0.135940 \t\tEpoch: 28  Time taken: 10.268678188323975\n",
            "| Epoch [ 29/ 50] Iter[391/390] : Loss:0.238539 \t\tEpoch: 29  Time taken: 10.336290121078491\n",
            "| Epoch [ 30/ 50] Iter[391/390] : Loss:0.051023 \t\tEpoch: 30  Time taken: 10.247560024261475\n",
            "| Epoch [ 31/ 50] Iter[391/390] : Loss:0.318944 \t\tEpoch: 31  Time taken: 10.411075592041016\n",
            "| Epoch [ 32/ 50] Iter[391/390] : Loss:0.070407 \t\tEpoch: 32  Time taken: 10.23029613494873\n",
            "| Epoch [ 33/ 50] Iter[391/390] : Loss:0.171403 \t\tEpoch: 33  Time taken: 10.391814708709717\n",
            "| Epoch [ 34/ 50] Iter[391/390] : Loss:0.266899 \t\tEpoch: 34  Time taken: 10.292010068893433\n",
            "| Epoch [ 35/ 50] Iter[391/390] : Loss:0.029155 \t\tEpoch: 35  Time taken: 10.375515460968018\n",
            "| Epoch [ 36/ 50] Iter[391/390] : Loss:0.149835 \t\tEpoch: 36  Time taken: 10.322794437408447\n",
            "| Epoch [ 37/ 50] Iter[391/390] : Loss:0.011222 \t\tEpoch: 37  Time taken: 10.468667030334473\n",
            "| Epoch [ 38/ 50] Iter[391/390] : Loss:0.111483 \t\tEpoch: 38  Time taken: 10.321659564971924\n",
            "| Epoch [ 39/ 50] Iter[391/390] : Loss:0.038336 \t\tEpoch: 39  Time taken: 10.24245309829712\n",
            "| Epoch [ 40/ 50] Iter[391/390] : Loss:0.022725 \t\tEpoch: 40  Time taken: 10.242920398712158\n",
            "| Epoch [ 41/ 50] Iter[391/390] : Loss:0.071848 \t\tEpoch: 41  Time taken: 10.346039772033691\n",
            "| Epoch [ 42/ 50] Iter[391/390] : Loss:0.639204 \t\tEpoch: 42  Time taken: 10.185934066772461\n",
            "| Epoch [ 43/ 50] Iter[391/390] : Loss:0.089703 \t\tEpoch: 43  Time taken: 10.421391248703003\n",
            "| Epoch [ 44/ 50] Iter[391/390] : Loss:0.073093 \t\tEpoch: 44  Time taken: 10.189579486846924\n",
            "| Epoch [ 45/ 50] Iter[391/390] : Loss:0.025296 \t\tEpoch: 45  Time taken: 10.385548830032349\n",
            "| Epoch [ 46/ 50] Iter[391/390] : Loss:0.136784 \t\tEpoch: 46  Time taken: 10.287485599517822\n",
            "| Epoch [ 47/ 50] Iter[391/390] : Loss:0.287022 \t\tEpoch: 47  Time taken: 10.312275171279907\n",
            "| Epoch [ 48/ 50] Iter[391/390] : Loss:0.156725 \t\tEpoch: 48  Time taken: 10.219827890396118\n",
            "| Epoch [ 49/ 50] Iter[391/390] : Loss:0.022465 \t\tEpoch: 49  Time taken: 10.421661615371704\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-6d3ebd3c6077>:241: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_name))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch [ 49/ 50] : Acc:96.835938 \t\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-6d3ebd3c6077>:285: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_name))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([9693., 9520., 9396., 9382.])\n",
            "tensor([9073., 9025., 9024., 9024.])\n"
          ]
        }
      ],
      "source": [
        "#gatminst.py\n",
        "#torch dependencies\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "\n",
        "##############################parse inputs###################\n",
        "import getopt\n",
        "import sys\n",
        "\n",
        "import os\n",
        "if not os.path.isdir('./results'):\n",
        "    os.mkdir('./results')\n",
        "if not os.path.isdir('./log'):\n",
        "    os.mkdir('./log')\n",
        "if not os.path.isdir('./models'):\n",
        "    os.mkdir('./models')\n",
        "if not os.path.isdir('./data'):\n",
        "    os.mkdir('./data')\n",
        "\n",
        "\n",
        "EXP_NAME = 'MNIST_MLeNet_GAT'\n",
        "MAX_EPOCHS = 50\n",
        "l_ce = 1.0\n",
        "l2_reg = 15.0\n",
        "mul = 3.0\n",
        "TRAIN_BATCH_SIZE = 128 #CHANGED\n",
        "Feps = 0.3\n",
        "B_val = 0.3\n",
        "lr_factor = 5.0\n",
        "\n",
        "LOG_FILE_NAME = 'log/' + EXP_NAME + '_training_log.txt'\n",
        "\n",
        "\n",
        "#Defaults\n",
        "l_ce = 1.0\n",
        "l2_reg = 15.0\n",
        "mul = 3.0\n",
        "TRAIN_BATCH_SIZE = 128 #CHANGED\n",
        "Feps = 0.3\n",
        "B_val = 0.3\n",
        "MAX_EPOCHS = 50\n",
        "lr_factor = 5.0\n",
        "\n",
        "###################################### Function Definitions #######################################\n",
        "\n",
        "def Guided_Attack(model,loss,image,target,eps=0.3,bounds=[0,1],steps=1,P_out=[],l2_reg=15):\n",
        "    tar = Variable(target.cuda())\n",
        "    img = image.cuda()\n",
        "    eps = eps/steps\n",
        "    for step in range(steps):\n",
        "        img = Variable(img,requires_grad=True)\n",
        "        img.grad = None\n",
        "        out  = model(img)\n",
        "        R_out = nn.Softmax(dim=1)(out)\n",
        "        cost = loss(out,tar) + l2_reg*(((P_out - R_out)**2.0).sum(1)).mean(0)\n",
        "        cost.backward()\n",
        "        per = eps * torch.sign(img.grad.data)\n",
        "        adv = img.data + per.cuda()\n",
        "        img = torch.clamp(adv,bounds[0],bounds[1])\n",
        "    return img\n",
        "\n",
        "def execfile(filepath):\n",
        "    with open(filepath, 'rb') as file:\n",
        "        exec(compile(file.read(), filepath, 'exec'))\n",
        "        globals().update(locals())\n",
        "\n",
        "\n",
        "\n",
        "#######################################Cudnn##############################################\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.benchmark=True\n",
        "print('Cudnn status:',torch.backends.cudnn.enabled)\n",
        "#######################################Set tensor to CUDA#########################################\n",
        "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "#######################################Parameters##################################################\n",
        "TRAIN_BATCH_SIZE = TRAIN_BATCH_SIZE\n",
        "\n",
        "VAL_BATCH_SIZE   = 1024\n",
        "TEST_BATCH_SIZE  = 1024\n",
        "BASE_LR          = 5e-3 #CHANGED FROM 1e-2\n",
        "MAX_ITER         = (MAX_EPOCHS*50000)/TRAIN_BATCH_SIZE\n",
        "MODEL_PREFIX     = 'models/mnist_'+EXP_NAME+'_epoch_'\n",
        "####################################### load network ################################################\n",
        "#execfile('MNIST_Network.py')\n",
        "model = MNIST_Network()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Automatically detect GPU or fallback to CPU\n",
        "model.to(device)\n",
        "\n",
        "model.train()\n",
        "###################################### load data ###################################################\n",
        "transform = transforms.Compose([\n",
        "        transforms.ToTensor(),])\n",
        "\n",
        "train_set  = torchvision.datasets.MNIST(root='./data', train=True , download=True, transform=transform)\n",
        "val_set    = torchvision.datasets.MNIST(root='./data', train=True , download=True, transform=transform)\n",
        "test_set   = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Split training into train and validation\n",
        "train_size = 50000\n",
        "valid_size = 10000\n",
        "test_size  = 10000\n",
        "\n",
        "#get indices seed\n",
        "np.random.seed(0)\n",
        "indices    = np.arange(train_size+valid_size)\n",
        "np.random.shuffle(indices)\n",
        "train_indices = indices[0:train_size]\n",
        "val_indices   = indices[train_size:]\n",
        "\n",
        "#get data loader ofr train val and test\n",
        "train_loader = torch.utils.data.DataLoader(train_set,batch_size=TRAIN_BATCH_SIZE ,sampler=SubsetRandomSampler(train_indices))\n",
        "val_loader   = torch.utils.data.DataLoader(val_set,sampler = SubsetRandomSampler(val_indices),batch_size=VAL_BATCH_SIZE)\n",
        "test_loader  = torch.utils.data.DataLoader(test_set,batch_size=TEST_BATCH_SIZE)\n",
        "print('MNIST dataloader: Done')\n",
        "###################################################################################################\n",
        "epochs    = MAX_EPOCHS\n",
        "iteration = 0\n",
        "loss      = nn.CrossEntropyLoss()\n",
        "loss_no_reduce = nn.CrossEntropyLoss(reduce=False)\n",
        "LR   = BASE_LR\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR,momentum=0.9,weight_decay=5e-4)\n",
        "optimizer.zero_grad()\n",
        "##################################################################################################\n",
        "for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "    iter_loss =0\n",
        "    counter =0\n",
        "\n",
        "    for data, target in train_loader:\n",
        "        data   = Variable(data).cuda()\n",
        "        target = Variable(target).cuda()\n",
        "\n",
        "        out  = model(data)\n",
        "        P_out = nn.Softmax(dim=1)(out)\n",
        "\n",
        "        adv_data = data + ((B_val)*torch.sign(torch.tensor([0.5]) - torch.rand_like(data)).cuda())\n",
        "        adv_data = torch.clamp(adv_data,0.0,1.0)\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        adv_data = Guided_Attack(model,loss,adv_data,target,eps=Feps,steps=1,P_out=P_out,l2_reg=l2_reg)\n",
        "\n",
        "        delta = adv_data - data\n",
        "        delta = torch.clamp(delta,-0.3,0.3)\n",
        "        adv_data = data+delta\n",
        "        adv_data = torch.clamp(adv_data,0.0,1.0)\n",
        "\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        adv_out  = model(adv_data)\n",
        "        out  = model(data)\n",
        "\n",
        "        Q_out = nn.Softmax(dim=1)(adv_out)\n",
        "        P_out = nn.Softmax(dim=1)(out)\n",
        "\n",
        "        '''LOSS COMPUTATION'''\n",
        "\n",
        "        closs = loss(out,target)\n",
        "\n",
        "        reg_loss =  ((P_out - Q_out)**2.0).sum(1).mean(0)\n",
        "\n",
        "        cost = l_ce*closs + l2_reg*reg_loss\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if iteration%100==0:\n",
        "            msg = 'iter,'+str(iteration)+',clean loss,'+str(closs.data.cpu().numpy()) \\\n",
        "            +',reg loss,'+str(reg_loss.data.cpu().numpy()) \\\n",
        "            +',total loss,'+str(cost.data.cpu().numpy()) \\\n",
        "                                        +'\\n'\n",
        "            log_file = open(LOG_FILE_NAME,'a+')\n",
        "            log_file.write(msg)\n",
        "            log_file.close()\n",
        "            model.train()\n",
        "            #print msg\n",
        "        iteration = iteration + 1\n",
        "        ##console log\n",
        "        counter = counter + 1\n",
        "        sys.stdout.write('\\r')\n",
        "        sys.stdout.write('| Epoch [%3d/%3d] Iter[%3d/%3d] : Loss:%f \\t\\t'\n",
        "                %(epoch, MAX_EPOCHS, counter,\n",
        "                    (train_size/TRAIN_BATCH_SIZE),cost.data.cpu().numpy()))\n",
        "    end = time.time()\n",
        "    print('Epoch:',epoch,' Time taken:',(end-start))\n",
        "\n",
        "    model_name = MODEL_PREFIX+str(epoch)+'.pkl'\n",
        "    torch.save(model.state_dict(),model_name)\n",
        "\n",
        "    if epoch in [epochs/4, 2*epochs/4, 3*epochs/4]:\n",
        "        LR /= lr_factor\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = LR\n",
        "        l2_reg = l2_reg*mul\n",
        "\n",
        "#######################################################################################################################\n",
        "model.eval()\n",
        "\n",
        "\n",
        "def FGSM_Attack_step(model,loss,image,target,eps=0.1,bounds=[0,1],GPU=0,steps=30):\n",
        "    tar = Variable(target.cuda())\n",
        "    img = image.cuda()\n",
        "    eps = eps/steps\n",
        "    for step in range(steps):\n",
        "        img = Variable(img,requires_grad=True)\n",
        "        img.grad = None\n",
        "        out  = model(img)\n",
        "        cost = loss(out,tar)\n",
        "        cost.backward()\n",
        "        per = eps * torch.sign(img.grad.data)\n",
        "        adv = img.data + per.cuda()\n",
        "        img = torch.clamp(adv,bounds[0],bounds[1])\n",
        "    return img\n",
        "\n",
        "\n",
        "##########################FIND BEST MODEL###############################################\n",
        "EVAL_LOG_NAME = 'results/'+EXP_NAME+'.txt'\n",
        "ACC_EPOCH_LOG_NAME = 'results/acc_'+EXP_NAME+'_epoch.txt'\n",
        "ACC_IFGSM_EPOCH_LOG_NAME = 'results/ifgsm_acc_'+EXP_NAME+'_epoch.txt'\n",
        "log_file = open(EVAL_LOG_NAME,'a')\n",
        "msg = '##################### iter.FGSM: steps=40,eps=0.3 ####################\\n'\n",
        "log_file.write(msg)\n",
        "log_file.close()\n",
        "accuracy_log = np.zeros(MAX_EPOCHS)\n",
        "for epoch in range(MAX_EPOCHS):\n",
        "    model_name = MODEL_PREFIX+str(epoch)+'.pkl'\n",
        "    model.load_state_dict(torch.load(model_name))\n",
        "    eps=0.3\n",
        "    accuracy = 0\n",
        "    accuracy_ifgsm = 0\n",
        "    i = 0\n",
        "    for data, target in val_loader:\n",
        "        data   = Variable(data).cuda()\n",
        "        target = Variable(target).cuda()\n",
        "        out = model(data)\n",
        "        prediction = out.data.max(1)[1]\n",
        "        accuracy = accuracy + prediction.eq(target.data).sum()\n",
        "        i = i + 1\n",
        "    for data, target in val_loader:\n",
        "        data = FGSM_Attack_step(model,loss,data,target,eps=eps,steps=40)\n",
        "        data   = Variable(data).cuda()\n",
        "        target = Variable(target).cuda()\n",
        "        out = model(data)\n",
        "        prediction = out.data.max(1)[1]\n",
        "        accuracy_ifgsm = accuracy_ifgsm + prediction.eq(target.data).sum()\n",
        "    acc = (accuracy.item()*1.0) / (i*VAL_BATCH_SIZE) * 100\n",
        "    acc_ifgsm = (accuracy_ifgsm.item()*1.0) / (i*VAL_BATCH_SIZE) * 100\n",
        "    #log accuracy to file\n",
        "    msg= str(epoch)+','+str(acc)+'\\n'\n",
        "    log_file = open(ACC_EPOCH_LOG_NAME,'a')\n",
        "    log_file.write(msg)\n",
        "    log_file.close()\n",
        "\n",
        "    msg1= str(epoch)+','+str(acc_ifgsm)+'\\n'\n",
        "    log_file = open(ACC_IFGSM_EPOCH_LOG_NAME,'a')\n",
        "    log_file.write(msg1)\n",
        "    log_file.close()\n",
        "\n",
        "    accuracy_log[epoch] = acc_ifgsm\n",
        "    sys.stdout.write('\\r')\n",
        "    sys.stdout.write('| Epoch [%3d/%3d] : Acc:%f \\t\\t'\n",
        "            %(epoch, MAX_EPOCHS,acc))\n",
        "    sys.stdout.flush()\n",
        "\n",
        "log_file = open(EVAL_LOG_NAME,'a')\n",
        "msg = 'Epoch,'+str(accuracy_log.argmax())+',Acc,'+str(accuracy_log.max())+'\\n'\n",
        "log_file.write(msg)\n",
        "log_file.close()\n",
        "\n",
        "model_name = MODEL_PREFIX+str(accuracy_log.argmax())+'.pkl'\n",
        "model.load_state_dict(torch.load(model_name))\n",
        "model.eval()\n",
        "model.cuda()\n",
        "##################################### FGSM #############################################\n",
        "EVAL_LOG_NAME = 'results/'+EXP_NAME+'.txt'\n",
        "log_file = open(EVAL_LOG_NAME,'a')\n",
        "msg = '##################### FGSM ####################\\n'\n",
        "log_file.write(msg)\n",
        "log_file.close()\n",
        "for eps in np.arange(0,0.301,0.05):\n",
        "    i = 0\n",
        "    accuracy = 0\n",
        "    for data, target in test_loader:\n",
        "        adv = FGSM_Attack_step(model,loss,data,target,eps=eps,steps=1)\n",
        "        data   = Variable(adv).cuda()\n",
        "        target = Variable(target).cuda()\n",
        "        out = model(data)\n",
        "        prediction = out.data.max(1)[1]\n",
        "        accuracy = accuracy + prediction.eq(target.data).sum()\n",
        "        i = i + 1\n",
        "    acc = (accuracy.item()*1.0) / (test_size) * 100\n",
        "    log_file = open(EVAL_LOG_NAME,'a')\n",
        "    msg = 'eps,'+str(eps)+',Acc,'+str(acc)+'\\n'\n",
        "    log_file.write(msg)\n",
        "    log_file.close()\n",
        "##################################### iFGSM #############################################\n",
        "log_file = open(EVAL_LOG_NAME,'a')\n",
        "msg = '##################### iFGSM: step=40 ####################\\n'\n",
        "log_file.write(msg)\n",
        "log_file.close()\n",
        "for eps in np.arange(0.05,0.301,0.05):\n",
        "    i = 0\n",
        "    accuracy = 0\n",
        "    for data, target in test_loader:\n",
        "        adv = FGSM_Attack_step(model,loss,data,target,eps=eps,steps=40)\n",
        "        data   = Variable(adv).cuda()\n",
        "        target = Variable(target).cuda()\n",
        "        out = model(data)\n",
        "        prediction = out.data.max(1)[1]\n",
        "        accuracy = accuracy + prediction.eq(target.data).sum()\n",
        "        i = i + 1\n",
        "    acc = (accuracy.item()*1.0) / (test_size) * 100\n",
        "    log_file = open(EVAL_LOG_NAME,'a')\n",
        "    msg = 'eps,'+str(eps)+',Acc,'+str(acc)+'\\n'\n",
        "    log_file.write(msg)\n",
        "    log_file.close()\n",
        "##################################### MSPGD #############################################\n",
        "\n",
        "def MSPGD(model,loss,data,target,eps=0.3,eps_iter=0.01,bounds=[],steps=[7,20,50,100,500]):\n",
        "    \"\"\"\n",
        "    model\n",
        "    loss : loss used for training\n",
        "    data : input to network\n",
        "    target : ground truth label corresponding to data\n",
        "    eps : perturbation srength added to image\n",
        "    eps_iter\n",
        "    \"\"\"\n",
        "    #Raise error if in training mode\n",
        "    if model.training:\n",
        "        assert 'Model is in  training mode'\n",
        "    tar = Variable(target.cuda())\n",
        "    data = data.cuda()\n",
        "    B,C,H,W = data.size()\n",
        "    noise  = torch.FloatTensor(np.random.uniform(-eps,eps,(B,C,H,W))).cuda()\n",
        "    noise  = torch.clamp(noise,-eps,eps)\n",
        "    img_arr = []\n",
        "    for step in range(steps[-1]):\n",
        "        # convert data and corresponding into cuda variable\n",
        "        img = data + noise\n",
        "        img = Variable(img,requires_grad=True)\n",
        "        # make gradient of img to zeros\n",
        "        img.grad = None\n",
        "        # forward pass\n",
        "        out  = model(img)\n",
        "        #compute loss using true label\n",
        "        cost = loss(out,tar)\n",
        "        #backward pass\n",
        "        cost.backward()\n",
        "        #get gradient of loss wrt data\n",
        "        per =  torch.sign(img.grad.data)\n",
        "        #convert eps 0-1 range to per channel range\n",
        "        per[:,0,:,:] = (eps_iter * (bounds[0,1] - bounds[0,0])) * per[:,0,:,:]\n",
        "        if(per.size(1)>1):\n",
        "            per[:,1,:,:] = (eps_iter * (bounds[1,1] - bounds[1,0])) * per[:,1,:,:]\n",
        "            per[:,2,:,:] = (eps_iter * (bounds[2,1] - bounds[2,0])) * per[:,2,:,:]\n",
        "        #  ascent\n",
        "        adv = img.data + per.cuda()\n",
        "        #clip per channel data out of the range\n",
        "        img.requires_grad =False\n",
        "        img[:,0,:,:] = torch.clamp(adv[:,0,:,:],bounds[0,0],bounds[0,1])\n",
        "        if(per.size(1)>1):\n",
        "            img[:,1,:,:] = torch.clamp(adv[:,1,:,:],bounds[1,0],bounds[1,1])\n",
        "            img[:,2,:,:] = torch.clamp(adv[:,2,:,:],bounds[2,0],bounds[2,1])\n",
        "        img = img.data\n",
        "        noise = img - data\n",
        "        noise  = torch.clamp(noise,-eps,eps)\n",
        "        for j in range(len(steps)):\n",
        "            if step == steps[j]-1:\n",
        "                img_tmp = data + noise\n",
        "                img_arr.append(img_tmp)\n",
        "                break\n",
        "    return img_arr\n",
        "\n",
        "##################################### PGD, steps=[20,40,100,500] #############################################\n",
        "log_file = open(EVAL_LOG_NAME,'a')\n",
        "msg = '##################### PGD: steps=[20,40,100,500],eps_iter=0.01 ####################\\n'\n",
        "log_file.write(msg)\n",
        "log_file.close()\n",
        "all_steps = [20,40,100,500]\n",
        "num_steps = len(all_steps)\n",
        "eps = 0.3\n",
        "i = 0\n",
        "acc_arr = torch.zeros((num_steps))\n",
        "for data, target in test_loader:\n",
        "    adv_arr = MSPGD(model,loss,data,target,eps=eps,eps_iter=0.01,bounds=np.array([[0,1],[0,1],[0,1]]),steps=all_steps)\n",
        "    target = Variable(target).cuda()\n",
        "    for j in range(num_steps):\n",
        "        data   = Variable(adv_arr[j]).cuda()\n",
        "        out = model(data)\n",
        "        prediction = out.data.max(1)[1]\n",
        "        acc_arr[j] = acc_arr[j] + prediction.eq(target.data).sum()\n",
        "    i = i + 1\n",
        "print(acc_arr)\n",
        "for j in range(num_steps):\n",
        "    acc_arr[j] = (acc_arr[j].item()*1.0) / (test_size) * 100\n",
        "    log_file = open(EVAL_LOG_NAME,'a')\n",
        "    msg = 'eps,'+str(eps)+',steps,'+str(all_steps[j])+',Acc,'+str(acc_arr[j])+'\\n'\n",
        "    log_file.write(msg)\n",
        "    log_file.close()\n",
        "\n",
        "\n",
        "\n",
        "def max_margin_loss(x,y):\n",
        "    B = y.size(0)\n",
        "    corr = x[range(B),y]\n",
        "\n",
        "    x_new = x - 1000*torch.eye(10)[y].cuda()\n",
        "    tar = x[range(B),x_new.argmax(dim=1)]\n",
        "    loss = tar - corr\n",
        "    loss = torch.mean(loss)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def GAMA_PGD(model,data,target,eps,eps_iter,bounds,steps,w_reg,lin,SCHED,drop):\n",
        "\n",
        "    #Raise error if in training mode\n",
        "    if model.training:\n",
        "        assert 'Model is in  training mode'\n",
        "    tar = Variable(target.cuda())\n",
        "    data = data.cuda()\n",
        "    B,C,H,W = data.size()\n",
        "    noise  = torch.FloatTensor(np.random.uniform(-eps,eps,(B,C,H,W))).cuda()\n",
        "    noise  = eps*torch.sign(noise)\n",
        "    img_arr = []\n",
        "    W_REG = w_reg\n",
        "    orig_img = data+noise\n",
        "    orig_img = Variable(orig_img,requires_grad=True)\n",
        "    for step in range(steps[-1]):\n",
        "        # convert data and corresponding into cuda variable\n",
        "        img = data + noise\n",
        "        img = Variable(img,requires_grad=True)\n",
        "\n",
        "        if step in SCHED:\n",
        "            eps_iter /= drop\n",
        "\n",
        "        # make gradient of img to zeros\n",
        "        img.grad = None\n",
        "        # forward pass\n",
        "        orig_out = model(orig_img)\n",
        "        P_out = nn.Softmax(dim=1)(orig_out)\n",
        "\n",
        "        out  = model(img)\n",
        "        Q_out = nn.Softmax(dim=1)(out)\n",
        "        #compute loss using true label\n",
        "        if step <= lin:\n",
        "            cost =  W_REG*((P_out - Q_out)**2.0).sum(1).mean(0) + max_margin_loss(Q_out,tar)\n",
        "            W_REG -= w_reg/lin\n",
        "        else:\n",
        "            cost = max_margin_loss(Q_out,tar)\n",
        "        #backward pass\n",
        "        cost.backward()\n",
        "        #get gradient of loss wrt data\n",
        "        per =  torch.sign(img.grad.data)\n",
        "        #convert eps 0-1 range to per channel range\n",
        "        per[:,0,:,:] = (eps_iter * (bounds[0,1] - bounds[0,0])) * per[:,0,:,:]\n",
        "        if(per.size(1)>1):\n",
        "            per[:,1,:,:] = (eps_iter * (bounds[1,1] - bounds[1,0])) * per[:,1,:,:]\n",
        "            per[:,2,:,:] = (eps_iter * (bounds[2,1] - bounds[2,0])) * per[:,2,:,:]\n",
        "        #  ascent\n",
        "        adv = img.data + per.cuda()\n",
        "        #clip per channel data out of the range\n",
        "        img.requires_grad =False\n",
        "        img[:,0,:,:] = torch.clamp(adv[:,0,:,:],bounds[0,0],bounds[0,1])\n",
        "        if(per.size(1)>1):\n",
        "            img[:,1,:,:] = torch.clamp(adv[:,1,:,:],bounds[1,0],bounds[1,1])\n",
        "            img[:,2,:,:] = torch.clamp(adv[:,2,:,:],bounds[2,0],bounds[2,1])\n",
        "        img = img.data\n",
        "        noise = img - data\n",
        "        noise  = torch.clamp(noise,-eps,eps)\n",
        "\n",
        "        for j in range(len(steps)):\n",
        "            if step == steps[j]-1:\n",
        "                img_tmp = data + noise\n",
        "                img_arr.append(img_tmp)\n",
        "                break\n",
        "    return img_arr\n",
        "\n",
        "\n",
        "\n",
        "SCHED = [50,75]\n",
        "drop = 10\n",
        "lin = 50\n",
        "w_reg = 5\n",
        "##################################### GAMA PGD, steps=[60,85,90,100] #############################################\n",
        "log_file = open(EVAL_LOG_NAME,'a+')\n",
        "msg = '##################### Gama-PGD Wreg5 lin50, drop by 10 at [50,75]: steps=[60,85,90,100], eps_iter_init=0.3  ####################\\n'\n",
        "log_file.write(msg)\n",
        "log_file.close()\n",
        "all_steps = [60,85,90,100]\n",
        "num_steps = len(all_steps)\n",
        "eps = 0.3\n",
        "i = 0\n",
        "acc_arr = torch.zeros((num_steps))\n",
        "for data, target in test_loader:\n",
        "    adv_arr = GAMA_PGD(model,data,target,eps=eps,eps_iter=0.3,bounds=np.array([[0,1],[0,1],[0,1]]),steps=all_steps,w_reg=w_reg,lin=lin,SCHED=SCHED,drop=drop)\n",
        "    target = Variable(target).cuda()\n",
        "    for j in range(num_steps):\n",
        "        data   = Variable(adv_arr[j]).cuda()\n",
        "        out = model(data)\n",
        "        prediction = out.data.max(1)[1]\n",
        "        acc_arr[j] = acc_arr[j] + prediction.eq(target.data).sum()\n",
        "    i = i + 1\n",
        "print(acc_arr)\n",
        "for j in range(num_steps):\n",
        "    acc_arr[j] = (acc_arr[j].item()*1.0) / (test_size) * 100\n",
        "    log_file = open(EVAL_LOG_NAME,'a+')\n",
        "    msg = 'eps,'+str(eps)+',steps,'+str(all_steps[j])+',Acc,'+str(acc_arr[j])+'\\n'\n",
        "    log_file.write(msg)\n",
        "    log_file.close()"
      ]
    }
  ]
}