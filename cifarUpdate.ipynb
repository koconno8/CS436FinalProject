{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koconno8/CS436FinalProject/blob/main/FinalProject436.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgewrWieVwje"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "import getopt\n",
        "import sys\n",
        "import os\n",
        "import logging\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZSQOyvYWB0O"
      },
      "outputs": [],
      "source": [
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "        self.relu = nn.ReLU(inplace=False)  # Add explicit ReLU module\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x) if self.shortcut else x\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        # Define ReLU modules with inplace=False\n",
        "        self.relu = nn.ReLU(inplace=False)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Use explicit ReLU module instead of functional\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x) if self.shortcut else x\n",
        "        out = self.relu(out)  # Final activation\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = conv3x3(3,64)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=False)  # Add explicit ReLU module\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # Replace F.avg_pool2d with nn.AdaptiveAvgPool2d\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.bn1(self.conv1(x)))  # Use module instead of functional\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avgpool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2,2,2,2])\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3,4,6,3])\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3,4,6,3])\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3,4,23,3])\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3,8,36,3])\n",
        "\n",
        "def test_resnet():\n",
        "    net = ResNet50()\n",
        "    y = net(Variable(torch.randn(1,3,32,32)))\n",
        "    print(y.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jtk-OriZW5ja"
      },
      "outputs": [],
      "source": [
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "# Read command-line arguments\n",
        "opts = sys.argv[1::2]\n",
        "args = sys.argv[2::2]\n",
        "\n",
        "# Default parameters\n",
        "l_ce = 1.0\n",
        "l2_reg = 10.0\n",
        "mul = 4.0\n",
        "TRAIN_BATCH_SIZE = 128 #Changed from 64 to 128 #CHANGE MADE\n",
        "Feps = 8.0\n",
        "B_val = 4.0\n",
        "MAX_EPOCHS = 50\n",
        "lr_factor = 10.0\n",
        "EXP_NAME = \"default_experiment\"\n",
        "LOG_FILE_NAME = os.path.join('log', f'{EXP_NAME}.txt')\n",
        "\n",
        "log_dir = os.path.dirname(LOG_FILE_NAME)\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "\n",
        "# Parse arguments and log the changes\n",
        "for i in range(len(opts)):\n",
        "    opt = opts[i]\n",
        "    arg = args[i]\n",
        "    if opt == '-EXP_NAME':\n",
        "        EXP_NAME = str(arg)\n",
        "        LOG_FILE_NAME = os.path.join('log', f'{EXP_NAME}.txt')\n",
        "        print('EXP_NAME:', EXP_NAME)\n",
        "    elif opt == '-MAX_EPOCHS':\n",
        "        MAX_EPOCHS = int(arg)\n",
        "        print('MAX_EPOCHS:', MAX_EPOCHS)\n",
        "    elif opt == '-l_ce':\n",
        "        l_ce = float(arg)\n",
        "        print('l_ce:', l_ce)\n",
        "    elif opt == '-B_val':\n",
        "        B_val = float(arg)\n",
        "        print('Initial Noise Magnitude:', B_val)\n",
        "    elif opt == '-l2_reg':\n",
        "        l2_reg = float(arg)\n",
        "        print('l2_reg:', l2_reg)\n",
        "    elif opt == '-b_size':\n",
        "        TRAIN_BATCH_SIZE = int(arg)\n",
        "        print('Training Batch Size:', TRAIN_BATCH_SIZE)\n",
        "    elif opt == '-Feps':\n",
        "        Feps = float(arg)\n",
        "        print('RFGSM Epsilon:', Feps)\n",
        "    elif opt == '-lr_factor':\n",
        "        lr_factor = float(arg)\n",
        "        print('lr_factor:', lr_factor)\n",
        "    elif opt == '-mul':\n",
        "        mul = float(arg)\n",
        "        print('Step Mult. factor:', mul)\n",
        "\n",
        "# Set up logging to the log file\n",
        "logging.basicConfig(\n",
        "    filename=LOG_FILE_NAME,\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "# Log experiment details\n",
        "logging.info(f\"Experiment Name: {EXP_NAME}\")\n",
        "logging.info(f\"MAX_EPOCHS: {MAX_EPOCHS}\")\n",
        "logging.info(f\"l_ce: {l_ce}\")\n",
        "logging.info(f\"B_val (Initial Noise Magnitude): {B_val}\")\n",
        "logging.info(f\"l2_reg: {l2_reg}\")\n",
        "logging.info(f\"TRAIN_BATCH_SIZE: {TRAIN_BATCH_SIZE}\")\n",
        "logging.info(f\"Feps (RFGSM Epsilon): {Feps}\")\n",
        "logging.info(f\"lr_factor: {lr_factor}\")\n",
        "logging.info(f\"mul (Step Multiplier Factor): {mul}\")\n",
        "\n",
        "# You can replace `writer.add_scalar(...)` calls with `logging.info(...)` or other logging calls as needed.\n",
        "# Example:\n",
        "logging.info(\"Experiment setup complete and ready to run.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQT8knCqae_Y",
        "outputId": "1dbfd5d0-da93-4156-8cf0-a6aab83c84d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cudnn status: True\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Size of train set: 49000\n",
            "Size of val set: 1000\n",
            "CIFAR10 dataloader: Done\n",
            "| Epoch [  0/ 50] Iter[383/382] : Loss:1.582171 \t\tEpoch: 0  Time taken: 44.56588268280029\n",
            "| Epoch [  1/ 50] Iter[383/382] : Loss:1.604499 \t\tEpoch: 1  Time taken: 44.02065086364746\n",
            "| Epoch [  2/ 50] Iter[383/382] : Loss:1.312807 \t\tEpoch: 2  Time taken: 44.07874393463135\n",
            "| Epoch [  3/ 50] Iter[383/382] : Loss:1.379934 \t\tEpoch: 3  Time taken: 44.017263650894165\n",
            "| Epoch [  4/ 50] Iter[383/382] : Loss:1.247452 \t\tEpoch: 4  Time taken: 44.26279950141907\n",
            "| Epoch [  5/ 50] Iter[383/382] : Loss:1.456808 \t\tEpoch: 5  Time taken: 44.018877267837524\n",
            "| Epoch [  6/ 50] Iter[383/382] : Loss:1.105199 \t\tEpoch: 6  Time taken: 44.13804650306702\n",
            "| Epoch [  7/ 50] Iter[383/382] : Loss:1.289301 \t\tEpoch: 7  Time taken: 44.138195753097534\n",
            "| Epoch [  8/ 50] Iter[383/382] : Loss:1.136593 \t\tEpoch: 8  Time taken: 43.96189045906067\n",
            "| Epoch [  9/ 50] Iter[383/382] : Loss:1.158688 \t\tEpoch: 9  Time taken: 44.381227016448975\n",
            "| Epoch [ 10/ 50] Iter[383/382] : Loss:1.191254 \t\tEpoch: 10  Time taken: 44.33488082885742\n",
            "| Epoch [ 11/ 50] Iter[383/382] : Loss:1.201393 \t\tEpoch: 11  Time taken: 44.214181423187256\n",
            "| Epoch [ 12/ 50] Iter[383/382] : Loss:1.201700 \t\tEpoch: 12  Time taken: 44.245564222335815\n",
            "| Epoch [ 13/ 50] Iter[383/382] : Loss:1.029239 \t\tEpoch: 13  Time taken: 44.039010524749756\n",
            "| Epoch [ 14/ 50] Iter[383/382] : Loss:1.080611 \t\tEpoch: 14  Time taken: 44.29027605056763\n",
            "| Epoch [ 15/ 50] Iter[383/382] : Loss:1.143846 \t\tEpoch: 15  Time taken: 43.9720196723938\n",
            "| Epoch [ 16/ 50] Iter[383/382] : Loss:1.079474 \t\tEpoch: 16  Time taken: 43.916664361953735\n",
            "| Epoch [ 17/ 50] Iter[383/382] : Loss:1.026281 \t\tEpoch: 17  Time taken: 44.28358817100525\n",
            "| Epoch [ 18/ 50] Iter[383/382] : Loss:1.078858 \t\tEpoch: 18  Time taken: 44.12868285179138\n",
            "| Epoch [ 19/ 50] Iter[383/382] : Loss:1.037923 \t\tEpoch: 19  Time taken: 44.128530979156494\n",
            "| Epoch [ 20/ 50] Iter[383/382] : Loss:1.021414 \t\tEpoch: 20  Time taken: 44.0262348651886\n",
            "| Epoch [ 21/ 50] Iter[383/382] : Loss:1.078832 \t\tEpoch: 21  Time taken: 44.06011199951172\n",
            "| Epoch [ 22/ 50] Iter[383/382] : Loss:1.018117 \t\tEpoch: 22  Time taken: 44.20482516288757\n",
            "| Epoch [ 23/ 50] Iter[383/382] : Loss:1.011622 \t\tEpoch: 23  Time taken: 44.10982918739319\n",
            "| Epoch [ 24/ 50] Iter[383/382] : Loss:1.079152 \t\tEpoch: 24  Time taken: 44.16070103645325\n",
            "| Epoch [ 25/ 50] Iter[383/382] : Loss:1.128745 \t\tEpoch: 25  Time taken: 44.334803104400635\n",
            "| Epoch [ 26/ 50] Iter[383/382] : Loss:1.109894 \t\tEpoch: 26  Time taken: 44.194260120391846\n",
            "| Epoch [ 27/ 50] Iter[383/382] : Loss:1.047198 \t\tEpoch: 27  Time taken: 44.01632857322693\n",
            "| Epoch [ 28/ 50] Iter[383/382] : Loss:1.007902 \t\tEpoch: 28  Time taken: 44.128527879714966\n",
            "| Epoch [ 29/ 50] Iter[383/382] : Loss:1.042256 \t\tEpoch: 29  Time taken: 44.3173623085022\n",
            "| Epoch [ 30/ 50] Iter[383/382] : Loss:1.137218 \t\tEpoch: 30  Time taken: 44.08557963371277\n",
            "| Epoch [ 31/ 50] Iter[383/382] : Loss:1.040009 \t\tEpoch: 31  Time taken: 44.207881689071655\n",
            "| Epoch [ 32/ 50] Iter[383/382] : Loss:1.026533 \t\tEpoch: 32  Time taken: 44.15049862861633\n",
            "| Epoch [ 33/ 50] Iter[383/382] : Loss:1.013013 \t\tEpoch: 33  Time taken: 44.01309823989868\n",
            "| Epoch [ 34/ 50] Iter[383/382] : Loss:0.957470 \t\tEpoch: 34  Time taken: 44.28035497665405\n",
            "| Epoch [ 35/ 50] Iter[383/382] : Loss:0.948645 \t\tEpoch: 35  Time taken: 44.10414457321167\n",
            "| Epoch [ 36/ 50] Iter[383/382] : Loss:1.001490 \t\tEpoch: 36  Time taken: 44.14620113372803\n",
            "| Epoch [ 37/ 50] Iter[383/382] : Loss:0.871689 \t\tEpoch: 37  Time taken: 44.29940438270569\n",
            "| Epoch [ 38/ 50] Iter[383/382] : Loss:0.994501 \t\tEpoch: 38  Time taken: 44.18915343284607\n",
            "| Epoch [ 39/ 50] Iter[383/382] : Loss:0.843854 \t\tEpoch: 39  Time taken: 43.937819957733154\n",
            "| Epoch [ 40/ 50] Iter[383/382] : Loss:1.045969 \t\tEpoch: 40  Time taken: 43.912803173065186\n",
            "| Epoch [ 41/ 50] Iter[383/382] : Loss:1.010193 \t\tEpoch: 41  Time taken: 44.13120436668396\n",
            "| Epoch [ 42/ 50] Iter[383/382] : Loss:0.957103 \t\tEpoch: 42  Time taken: 44.093369483947754\n",
            "| Epoch [ 43/ 50] Iter[383/382] : Loss:0.919569 \t\tEpoch: 43  Time taken: 44.272488594055176\n",
            "| Epoch [ 44/ 50] Iter[383/382] : Loss:0.910121 \t\tEpoch: 44  Time taken: 44.06416058540344\n",
            "| Epoch [ 45/ 50] Iter[383/382] : Loss:1.036867 \t\tEpoch: 45  Time taken: 44.23716402053833\n",
            "| Epoch [ 46/ 50] Iter[383/382] : Loss:1.089679 \t\tEpoch: 46  Time taken: 44.599432706832886\n",
            "| Epoch [ 47/ 50] Iter[383/382] : Loss:0.970461 \t\tEpoch: 47  Time taken: 44.17726540565491\n",
            "| Epoch [ 48/ 50] Iter[383/382] : Loss:0.934894 \t\tEpoch: 48  Time taken: 44.31693720817566\n",
            "| Epoch [ 49/ 50] Iter[383/382] : Loss:1.013718 \t\tEpoch: 49  Time taken: 44.383116483688354\n"
          ]
        }
      ],
      "source": [
        "###################################### Function Definitions #######################################\n",
        "\n",
        "def Guided_Attack(model,loss,image,target,eps=8/255,bounds=[0,1],steps=1,P_out=[],l2_reg=10,alt=1):\n",
        "    tar = Variable(target.cuda())\n",
        "    img = image.cuda()\n",
        "    eps = eps/steps\n",
        "    for step in range(steps):\n",
        "        img = Variable(img,requires_grad=True)\n",
        "        if img.grad is not None:\n",
        "          img.grad.zero_()\n",
        "        out  = model(img)\n",
        "        R_out = nn.Softmax(dim=1)(out)\n",
        "        cost = loss(out,tar) + alt*l2_reg*(((P_out - R_out)**2.0).sum(1)).mean(0)\n",
        "        cost.backward()\n",
        "        per = eps * torch.sign(img.grad.data)\n",
        "        adv = img.data + per.cuda()\n",
        "        img = torch.clamp(adv,bounds[0],bounds[1])\n",
        "    return img\n",
        "\n",
        "\n",
        "#######################################Cudnn##############################################\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.benchmark=True\n",
        "print('Cudnn status:',torch.backends.cudnn.enabled)\n",
        "#######################################Set tensor to CUDA#########################################\n",
        "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "#######################################Parameters##################################################\n",
        "TRAIN_BATCH_SIZE = TRAIN_BATCH_SIZE\n",
        "VAL_BATCH_SIZE   = 128\n",
        "TEST_BATCH_SIZE   = 128\n",
        "BASE_LR          = 0.05 #Change made from 1e-1 to 0.05 #CHANGE MADE\n",
        "MAX_ITER         = (MAX_EPOCHS*50000)/TRAIN_BATCH_SIZE\n",
        "MODEL_PREFIX     = 'models/' + EXP_NAME + '_'\n",
        "\n",
        "####################################### load network ################################################\n",
        "model = ResNet18()\n",
        "model.cuda()\n",
        "model.train()\n",
        "\n",
        "###################################### load data ####################################################\n",
        "transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(size=32,padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),])\n",
        "\n",
        "train_set  = torchvision.datasets.CIFAR10(root='./data', train=True , download=True, transform=transform_train)\n",
        "val_set    = torchvision.datasets.CIFAR10(root='./data', train=True , download=True, transform=transform_test)\n",
        "test_set   = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "# Creating Train-Val split from original Training set\n",
        "train_size = 49000\n",
        "valid_size = 1000\n",
        "test_size  = 10000\n",
        "\n",
        "train_indices = list(range(50000))\n",
        "val_indices = []\n",
        "count = np.zeros(10)\n",
        "for index in range(len(train_set)):\n",
        "    _, target = train_set[index]\n",
        "    if(np.all(count==100)):\n",
        "        break\n",
        "    if(count[target]<100):\n",
        "        count[target] += 1\n",
        "        val_indices.append(index)\n",
        "        train_indices.remove(index)\n",
        "\n",
        "print(\"Size of train set:\",len(train_indices))\n",
        "print(\"Size of val set:\",len(val_indices))\n",
        "\n",
        "#get data loader for train, val and test\n",
        "train_loader = torch.utils.data.DataLoader(train_set,batch_size=TRAIN_BATCH_SIZE ,sampler=SubsetRandomSampler(train_indices))\n",
        "val_loader   = torch.utils.data.DataLoader(val_set,sampler = SubsetRandomSampler(val_indices),batch_size=VAL_BATCH_SIZE)\n",
        "test_loader   = torch.utils.data.DataLoader(test_set,batch_size=TEST_BATCH_SIZE)\n",
        "print('CIFAR10 dataloader: Done')\n",
        "\n",
        "###################################################################################################\n",
        "epochs    = MAX_EPOCHS\n",
        "iteration = 0\n",
        "loss      = nn.CrossEntropyLoss()\n",
        "LR = BASE_LR\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR,momentum=0.9,weight_decay=5e-4)\n",
        "optimizer.zero_grad()\n",
        "\n",
        "##################################################################################################\n",
        "for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "    iter_loss =0\n",
        "    counter =0\n",
        "    total_ce_loss = torch.tensor([0.0])\n",
        "    total_reg_loss = torch.tensor([0.0])\n",
        "\n",
        "    for data, target in train_loader:\n",
        "        data   = Variable(data).cuda()\n",
        "        target = Variable(target).cuda()\n",
        "\n",
        "        out  = model(data)\n",
        "        P_out = nn.Softmax(dim=1)(out)\n",
        "\n",
        "        adv_data = data + ((B_val/255.0)*torch.sign(torch.tensor([0.5]) - torch.rand_like(data)).cuda())\n",
        "        adv_data = torch.clamp(adv_data,0.0,1.0)\n",
        "\n",
        "        model.eval()\n",
        "        adv_data = Guided_Attack(model,loss,adv_data,target,eps=Feps/255.0,steps=1,P_out=P_out,l2_reg=l2_reg,alt=(counter%2))\n",
        "\n",
        "        delta = adv_data - data\n",
        "        delta = torch.clamp(delta,-8.0/255.0,8.0/255)\n",
        "        adv_data = data+delta\n",
        "        adv_data = torch.clamp(adv_data,0.0,1.0)\n",
        "\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        adv_out  = model(adv_data)\n",
        "        out  = model(data)\n",
        "\n",
        "        Q_out = nn.Softmax(dim=1)(adv_out)\n",
        "        P_out = nn.Softmax(dim=1)(out)\n",
        "\n",
        "        '''LOSS COMPUTATION'''\n",
        "        closs = loss(out,target)\n",
        "        reg_loss = ((P_out - Q_out)**2.0).sum(1).mean(0)\n",
        "        cost = l_ce*closs + l2_reg*reg_loss\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_ce_loss += closs.data\n",
        "        total_reg_loss += l2_reg*reg_loss.data\n",
        "\n",
        "        if iteration%100==0:\n",
        "            msg = 'iter,'+str(iteration)+',clean loss,'+str(closs.data.cpu().numpy()) \\\n",
        "            +',reg loss,'+str(reg_loss.data.cpu().numpy()) \\\n",
        "            +',total loss,'+str(cost.data.cpu().numpy()) \\\n",
        "            +'\\n'\n",
        "            log_file = open(LOG_FILE_NAME,'a+')\n",
        "            log_file.write(msg)\n",
        "            log_file.close()\n",
        "            model.train()\n",
        "\n",
        "        iteration = iteration + 1\n",
        "        counter = counter + 1\n",
        "        sys.stdout.write('\\r')\n",
        "        sys.stdout.write('| Epoch [%3d/%3d] Iter[%3d/%3d] : Loss:%f \\t\\t'\n",
        "                %(epoch, MAX_EPOCHS, counter,\n",
        "                    (train_size/TRAIN_BATCH_SIZE),cost.data.cpu().numpy()))\n",
        "    end = time.time()\n",
        "    print('Epoch:',epoch,' Time taken:',(end-start))\n",
        "\n",
        "    model_name = MODEL_PREFIX+str(epoch)+'.pkl'\n",
        "    torch.save(model.state_dict(),model_name)\n",
        "\n",
        "    logging.info(f\"Epoch {epoch} - Train CE Loss: {total_ce_loss}\")\n",
        "    logging.info(f\"Epoch {epoch} - Train Reg Loss: {total_reg_loss}\")\n",
        "\n",
        "    if epoch in [70,85]:\n",
        "        LR /= lr_factor\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = LR\n",
        "    if epoch == 85:\n",
        "        l2_reg = l2_reg*mul\n",
        "\n",
        "#######################################################################################################################\n",
        "model.eval()\n",
        "\n",
        "def FGSM_Attack_step(model,loss,image,target,eps=8/255,bounds=[0,1],steps=30):\n",
        "    tar = Variable(target.cuda())\n",
        "    img = image.cuda()\n",
        "    eps = eps/steps\n",
        "    for step in range(steps):\n",
        "        img = Variable(img,requires_grad=True)\n",
        "        if img.grad is not None:\n",
        "            img.grad.zero_()\n",
        "        out  = model(img)\n",
        "        cost = loss(out,tar)\n",
        "        cost.backward()\n",
        "        per = eps * torch.sign(img.grad.data)\n",
        "        adv = img.data + per.cuda()\n",
        "        img = torch.clamp(adv,bounds[0],bounds[1])\n",
        "    return img\n",
        "\n",
        "def MSPGD(model,loss,data,target,eps=8/255,eps_iter=2/255,bounds=[],steps=[7,20,50,100,500]):\n",
        "    \"\"\"\n",
        "    model\n",
        "    loss : loss used for training\n",
        "    data : input to network\n",
        "    target : ground truth label corresponding to data\n",
        "    eps : perturbation srength added to image\n",
        "    eps_iter\n",
        "    \"\"\"\n",
        "    if model.training:\n",
        "        assert 'Model is in  training mode'\n",
        "    tar = Variable(target.cuda())\n",
        "    data = data.cuda()\n",
        "    B,C,H,W = data.size()\n",
        "    noise  = torch.FloatTensor(np.random.uniform(-eps,eps,(B,C,H,W))).cuda()\n",
        "    noise  = torch.clamp(noise,-eps,eps)\n",
        "    img_arr = []\n",
        "    for step in range(steps[-1]):\n",
        "        img = data + noise\n",
        "        img = Variable(img,requires_grad=True)\n",
        "        if img.grad is not None:\n",
        "            img.grad.zero_()\n",
        "        out  = model(img)\n",
        "        cost = loss(out,tar)\n",
        "        cost.backward()\n",
        "        per =  torch.sign(img.grad.data)\n",
        "        per[:,0,:,:] = (eps_iter * (bounds[0,1] - bounds[0,0])) * per[:,0,:,:]\n",
        "        if(per.size(1)>1):\n",
        "            per[:,1,:,:] = (eps_iter * (bounds[1,1] - bounds[1,0])) * per[:,1,:,:]\n",
        "            per[:,2,:,:] = (eps_iter * (bounds[2,1] - bounds[2,0])) * per[:,2,:,:]\n",
        "        adv = img.data + per.cuda()\n",
        "        img.requires_grad =False\n",
        "        img[:,0,:,:] = torch.clamp(adv[:,0,:,:],bounds[0,0],bounds[0,1])\n",
        "        if(per.size(1)>1):\n",
        "            img[:,1,:,:] = torch.clamp(adv[:,1,:,:],bounds[1,0],bounds[1,1])\n",
        "            img[:,2,:,:] = torch.clamp(adv[:,2,:,:],bounds[2,0],bounds[2,1])\n",
        "        img = img.data\n",
        "        noise = img - data\n",
        "        noise  = torch.clamp(noise,-eps,eps)\n",
        "        for j in range(len(steps)):\n",
        "            if step == steps[j]-1:\n",
        "                img_tmp = data + noise\n",
        "                img_arr.append(img_tmp)\n",
        "                break\n",
        "    return img_arr\n",
        "\n",
        "def max_margin_loss(x,y):\n",
        "    B = y.size(0)\n",
        "    corr = x[range(B),y]\n",
        "    x_new = x - 1000*torch.eye(10)[y].cuda()\n",
        "    tar = x[range(B),x_new.argmax(dim=1)]\n",
        "    loss = tar - corr\n",
        "    loss = torch.mean(loss)\n",
        "    return loss\n",
        "\n",
        "def GAMA_PGD(model, data, target, eps, eps_iter, bounds, steps, w_reg, lin, SCHED, drop):\n",
        "    #Raise error if in training mode\n",
        "    if model.training:\n",
        "        assert 'Model is in  training mode'\n",
        "    tar = Variable(target.cuda())\n",
        "    data = data.cuda()\n",
        "    B,C,H,W = data.size()\n",
        "    noise  = torch.FloatTensor(np.random.uniform(-eps,eps,(B,C,H,W))).cuda()\n",
        "    noise  = eps*torch.sign(noise)\n",
        "    img_arr = []\n",
        "    W_REG = w_reg\n",
        "    orig_img = data+noise\n",
        "    orig_img = Variable(orig_img,requires_grad=True)\n",
        "    for step in range(steps[-1]):\n",
        "        # convert data and corresponding into cuda variable\n",
        "        img = data + noise\n",
        "        img = Variable(img,requires_grad=True)\n",
        "\n",
        "        if step in SCHED:\n",
        "            eps_iter /= drop\n",
        "\n",
        "        # make gradient of img to zeros using .zero_()\n",
        "        if img.grad is not None:\n",
        "            img.grad.zero_()\n",
        "\n",
        "        # forward pass\n",
        "        orig_out = model(orig_img)\n",
        "        P_out = nn.Softmax(dim=1)(orig_out)\n",
        "\n",
        "        out  = model(img)\n",
        "        Q_out = nn.Softmax(dim=1)(out)\n",
        "        #compute loss using true label\n",
        "        if step <= lin:\n",
        "            cost =  W_REG*((P_out - Q_out)**2.0).sum(1).mean(0) + max_margin_loss(Q_out,tar)\n",
        "            W_REG -= w_reg/lin\n",
        "        else:\n",
        "            cost = max_margin_loss(Q_out,tar)\n",
        "        #backward pass\n",
        "        cost.backward()\n",
        "        #get gradient of loss wrt data\n",
        "        per =  torch.sign(img.grad.data)\n",
        "        #convert eps 0-1 range to per channel range\n",
        "        per[:,0,:,:] = (eps_iter * (bounds[0,1] - bounds[0,0])) * per[:,0,:,:]\n",
        "        if(per.size(1)>1):\n",
        "            per[:,1,:,:] = (eps_iter * (bounds[1,1] - bounds[1,0])) * per[:,1,:,:]\n",
        "            per[:,2,:,:] = (eps_iter * (bounds[2,1] - bounds[2,0])) * per[:,2,:,:]\n",
        "        #  ascent\n",
        "        adv = img.data + per.cuda()\n",
        "        #clip per channel data out of the range\n",
        "        img.requires_grad =False\n",
        "        img[:,0,:,:] = torch.clamp(adv[:,0,:,:],bounds[0,0],bounds[0,1])\n",
        "        if(per.size(1)>1):\n",
        "            img[:,1,:,:] = torch.clamp(adv[:,1,:,:],bounds[1,0],bounds[1,1])\n",
        "            img[:,2,:,:] = torch.clamp(adv[:,2,:,:],bounds[2,0],bounds[2,1])\n",
        "        img = img.data\n",
        "        noise = img - data\n",
        "        noise  = torch.clamp(noise,-eps,eps)\n",
        "\n",
        "        for j in range(len(steps)):\n",
        "            if step == steps[j]-1:\n",
        "                img_tmp = data + noise\n",
        "                img_arr.append(img_tmp)\n",
        "                break\n",
        "    return img_arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8aGFZCBR2It",
        "outputId": "67d268b2-874f-477b-fbaa-c4ee7593e62b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-eee7d28eb26b>:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_name))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch [ 49/ 50] : Acc:79.003906 \t\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-eee7d28eb26b>:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_name))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([4537., 4351., 4341., 4339., 4341.])\n",
            "tensor([4153., 3962., 3962., 3961.])\n"
          ]
        }
      ],
      "source": [
        "##########################FIND BEST MODEL###############################################\n",
        "os.makedirs('results', exist_ok=True)\n",
        "EVAL_LOG_NAME = 'results/'+EXP_NAME+'.txt'\n",
        "ACC_EPOCH_LOG_NAME = 'results/'+EXP_NAME+'acc_epoch.txt'\n",
        "ACC_IFGSM_EPOCH_LOG_NAME = 'results/'+EXP_NAME+'ifgsm_acc_epoch.txt'\n",
        "log_file = open(EVAL_LOG_NAME,'a+')\n",
        "msg = '##################### iter.FGSM: steps=7,eps=8.0/255####################\\n'\n",
        "log_file.write(msg)\n",
        "log_file.close()\n",
        "accuracy_log = np.zeros(MAX_EPOCHS)\n",
        "for epoch in range(MAX_EPOCHS):\n",
        "    model_name = MODEL_PREFIX+str(epoch)+'.pkl'\n",
        "    model.load_state_dict(torch.load(model_name))\n",
        "    eps=8.0/255\n",
        "    accuracy = 0\n",
        "    accuracy_ifgsm = 0\n",
        "    i = 0\n",
        "    for data, target in val_loader:\n",
        "        data   = Variable(data).cuda()\n",
        "        target = Variable(target).cuda()\n",
        "        out = model(data)\n",
        "        prediction = out.data.max(1)[1]\n",
        "        accuracy = accuracy + prediction.eq(target.data).sum()\n",
        "        i = i + 1\n",
        "    for data, target in val_loader:\n",
        "        data = FGSM_Attack_step(model,loss,data,target,eps=eps,steps=7)\n",
        "        data   = Variable(data).cuda()\n",
        "        target = Variable(target).cuda()\n",
        "        out = model(data)\n",
        "        prediction = out.data.max(1)[1]\n",
        "        accuracy_ifgsm = accuracy_ifgsm + prediction.eq(target.data).sum()\n",
        "    acc = (accuracy.item()*1.0) / (i*VAL_BATCH_SIZE) * 100\n",
        "    acc_ifgsm = (accuracy_ifgsm.item()*1.0) / (i*VAL_BATCH_SIZE) * 100\n",
        "    #log accuracy to file\n",
        "    msg= str(epoch)+','+str(acc)+'\\n'\n",
        "    log_file = open(ACC_EPOCH_LOG_NAME,'a+')\n",
        "    log_file.write(msg)\n",
        "    log_file.close()\n",
        "    logging.info(f\"Epoch {epoch}: Clean Validation Acc: {acc}\")\n",
        "\n",
        "\n",
        "    msg1= str(epoch)+','+str(acc_ifgsm)+'\\n'\n",
        "    log_file = open(ACC_IFGSM_EPOCH_LOG_NAME,'a+')\n",
        "    log_file.write(msg1)\n",
        "    log_file.close()\n",
        "    logging.info(f\"Epoch {epoch}: IFGSM-7 Validation Acc: {acc_ifgsm}\")\n",
        "\n",
        "    accuracy_log[epoch] = acc_ifgsm\n",
        "    sys.stdout.write('\\r')\n",
        "    sys.stdout.write('| Epoch [%3d/%3d] : Acc:%f \\t\\t'\n",
        "            %(epoch, MAX_EPOCHS,acc))\n",
        "    sys.stdout.flush()\n",
        "\n",
        "log_file = open(EVAL_LOG_NAME,'a+')\n",
        "msg = 'Epoch,'+str(accuracy_log.argmax())+',Acc,'+str(accuracy_log.max())+'\\n'\n",
        "log_file.write(msg)\n",
        "log_file.close()\n",
        "\n",
        "model_name = MODEL_PREFIX+str(accuracy_log.argmax())+'.pkl'\n",
        "model.load_state_dict(torch.load(model_name))\n",
        "model.eval()\n",
        "model.cuda()\n",
        "##################################### FGSM #############################################\n",
        "EVAL_LOG_NAME = 'results/'+EXP_NAME+'.txt'\n",
        "log_file = open(EVAL_LOG_NAME,'a+')\n",
        "msg = '##################### FGSM ####################\\n'\n",
        "log_file.write(msg)\n",
        "log_file.close()\n",
        "for eps in np.arange(0.0/255,10.0/255,2.0/255):\n",
        "    i = 0\n",
        "    accuracy = 0\n",
        "    for data, target in test_loader:\n",
        "        adv = FGSM_Attack_step(model,loss,data,target,eps=eps,steps=1)\n",
        "        data   = Variable(adv).cuda()\n",
        "        target = Variable(target).cuda()\n",
        "        out = model(data)\n",
        "        prediction = out.data.max(1)[1]\n",
        "        accuracy = accuracy + prediction.eq(target.data).sum()\n",
        "        i = i + 1\n",
        "    acc = (accuracy.item()*1.0) / (test_size) * 100\n",
        "    log_file = open(EVAL_LOG_NAME,'a+')\n",
        "    msg = 'eps,'+str(eps)+',Acc,'+str(acc)+'\\n'\n",
        "    log_file.write(msg)\n",
        "    log_file.close()\n",
        "##################################### iFGSM #############################################\n",
        "log_file = open(EVAL_LOG_NAME,'a+')\n",
        "msg = '##################### iFGSM: step=7 ####################\\n'\n",
        "log_file.write(msg)\n",
        "log_file.close()\n",
        "for eps in np.arange(2.0/255,10.0/255,2.0/255):\n",
        "    i = 0\n",
        "    accuracy = 0\n",
        "    for data, target in test_loader:\n",
        "        adv = FGSM_Attack_step(model,loss,data,target,eps=eps,steps=7)\n",
        "        data   = Variable(adv).cuda()\n",
        "        target = Variable(target).cuda()\n",
        "        out = model(data)\n",
        "        prediction = out.data.max(1)[1]\n",
        "        accuracy = accuracy + prediction.eq(target.data).sum()\n",
        "        i = i + 1\n",
        "    acc = (accuracy.item()*1.0) / (test_size) * 100\n",
        "    log_file = open(EVAL_LOG_NAME,'a+')\n",
        "    msg = 'eps,'+str(eps)+',Acc,'+str(acc)+'\\n'\n",
        "    log_file.write(msg)\n",
        "    log_file.close()\n",
        "\n",
        "\n",
        "##################################### PGD, steps=[7,20,50,100,500] #############################################\n",
        "log_file = open(EVAL_LOG_NAME,'a+')\n",
        "msg = '##################### PGD: steps=[7,20,50,100,500],eps_iter=2/255 ####################\\n'\n",
        "log_file.write(msg)\n",
        "log_file.close()\n",
        "all_steps = [7,20,50,100,500]\n",
        "num_steps = len(all_steps)\n",
        "eps = 8.0/255\n",
        "i = 0\n",
        "acc_arr = torch.zeros((num_steps))\n",
        "for data, target in test_loader:\n",
        "    adv_arr = MSPGD(model,loss,data,target,eps=eps,eps_iter=2.0/255,bounds=np.array([[0,1],[0,1],[0,1]]),steps=all_steps)\n",
        "    target = Variable(target).cuda()\n",
        "    for j in range(num_steps):\n",
        "        data   = Variable(adv_arr[j]).cuda()\n",
        "        out = model(data)\n",
        "        prediction = out.data.max(1)[1]\n",
        "        acc_arr[j] = acc_arr[j] + prediction.eq(target.data).sum()\n",
        "    i = i + 1\n",
        "print(acc_arr)\n",
        "for j in range(num_steps):\n",
        "    acc_arr[j] = (acc_arr[j].item()*1.0) / (test_size) * 100\n",
        "    log_file = open(EVAL_LOG_NAME,'a+')\n",
        "    msg = 'eps,'+str(eps)+',steps,'+str(all_steps[j])+',Acc,'+str(acc_arr[j])+'\\n'\n",
        "    log_file.write(msg)\n",
        "    log_file.close()\n",
        "\n",
        "\n",
        "SCHED = [60,85]\n",
        "drop = 10\n",
        "lin = 25\n",
        "w_reg = 50\n",
        "##################################### GAMA PGD, steps=[60,85,90,100] #############################################\n",
        "log_file = open(EVAL_LOG_NAME,'a+')\n",
        "msg = '##################### Gama-PGD Wreg50 lin25, drop by 10 at [60,85]: steps=[60,85,90,100], eps_iter_init=16/255  ####################\\n'\n",
        "log_file.write(msg)\n",
        "log_file.close()\n",
        "all_steps = [60,85,90,100]\n",
        "num_steps = len(all_steps)\n",
        "eps = 8.0/255\n",
        "i = 0\n",
        "acc_arr = torch.zeros((num_steps))\n",
        "for data, target in test_loader:\n",
        "    adv_arr = GAMA_PGD(model,data,target,eps=eps,eps_iter=16/255,bounds=np.array([[0,1],[0,1],[0,1]]),steps=all_steps,w_reg=w_reg,lin=lin,SCHED=SCHED,drop=drop)\n",
        "    target = Variable(target).cuda()\n",
        "    for j in range(num_steps):\n",
        "        data   = Variable(adv_arr[j]).cuda()\n",
        "        out = model(data)\n",
        "        prediction = out.data.max(1)[1]\n",
        "        acc_arr[j] = acc_arr[j] + prediction.eq(target.data).sum()\n",
        "    i = i + 1\n",
        "print(acc_arr)\n",
        "for j in range(num_steps):\n",
        "    acc_arr[j] = (acc_arr[j].item()*1.0) / (test_size) * 100\n",
        "    log_file = open(EVAL_LOG_NAME,'a+')\n",
        "    msg = 'eps,'+str(eps)+',steps,'+str(all_steps[j])+',Acc,'+str(acc_arr[j])+'\\n'\n",
        "    log_file.write(msg)\n",
        "    log_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZMadkcMsFVHm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
